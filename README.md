# riscv-paper-experiments

## RTL Simulation Quickstart

Let's start by spinning up the `snitch-toolchain` Docker container and mounting
a clone of this repo inside it at `/src`:

```shell
$ git clone https://github.com/opencompl/riscv-paper-experiments.git
$ docker run -ti --volume ${PWD}/riscv-paper-experiments:/src ghcr.io/nazavode/snitch-toolchain bash
```

*Note: `opencompl` members seems not to have enough rights to push packages to the organization's
package registry. The image built from [`snitch/docker/Dockerfile`](snitch/docker/Dockerfile) is currently made available at:
[`ghcr.io/nazavode/snitch-toolchain:latest`](https://github.com/users/nazavode/packages/container/package/snitch-toolchain)*

*Note: if on macOS, be aware that the official Docker app is trash, [OrbStack](https://orbstack.dev/) is highly recommended.*

*Note: if you're running Docker on an architecture other than `x86_64` (e.g.: Apple Silicon),
it's likely that your `docker run` command will complain about the image being `linux/amd64`.
Add the following option to explicitly ask for a specific platform and avoid issues:*

```shell
$docker run --platform linux/amd64 ...
```

To run binaries on Snitch we need to firsly build both the Snitch and MLIR runtimes:

```shell
$ cd /src/snitch/snitch-runtime
$ make install
$ cd /src/snitch/mlir-runtime
$ make install
```

To build a RISC-V executable, start from one of the kernels:

```shell
$ cd /src/kernels/addf/13x26xf32/
$ make addf_13x26xf32_mliropt.x
$ ls *.x
addf_13x26xf32_mliropt.x
```

The [Makefile](kernels/addf/13x26xf32/Makefile) performs the following steps:

1. `.mlir` -> `.ll.mlir` (a.k.a. MLIR source that uses the LLVM dialect only) via `mlir-opt`
2. `.ll.mlir` -> `.ll` via `mlir-translate`
3. `.ll` -> `.ll12` (a.k.a. LLVM IR backported to a form compatible with LLVM 12) via [`tollvm12.py`](snitch/tollvm12.py)
4. `.ll12` -> `.o` via `clang`
5. `.o` -> `.x` via `lld`, linking both runtimes and using the correct linker script to lay out the ELF file.

Once the ELF executable is ready, we can simulate its execution on a Snitch
cluster via the RTL simulator generated by Verilator:

```shell
$ make run_addf_13x26xf32_mliropt.x
/opt/snitch-rtl/bin/snitch_cluster.vlt addf_13x26xf32_mliropt.x
Warning: Failed to write binary name to logs/.rtlbinary
Wrote 36 bytes of bootrom to 0x1000
Wrote entry point 0x800006f0 to bootloader slot 0x1020
Wrote 38 bytes of bootdata to 0x1024
[Tracer] Logging Hart          8 to logs/trace_hart_00000008.dasm
[Tracer] Logging Hart          0 to logs/trace_hart_00000000.dasm
[Tracer] Logging Hart          1 to logs/trace_hart_00000001.dasm
[Tracer] Logging Hart          2 to logs/trace_hart_00000002.dasm
[Tracer] Logging Hart          3 to logs/trace_hart_00000003.dasm
[Tracer] Logging Hart          4 to logs/trace_hart_00000004.dasm
[Tracer] Logging Hart          5 to logs/trace_hart_00000005.dasm
[Tracer] Logging Hart          6 to logs/trace_hart_00000006.dasm
[Tracer] Logging Hart          7 to logs/trace_hart_00000007.dasm
TEST PASS, 2892, 1427, 338
$ echo $?
0
```

*Note: while the `main` function is run by all the compute cores in the cluster,
the current startup code **returns the integer return value of the core no. 0 only**,
**return values of cores other than no. 0 are discarded**.*

To disassemble and decode the execution traces:

```shell
$ make traces
$ tail -18 logs/trace_hart_00000000.txt
Performance metrics for section 0 @ (12, 27807):
snitch_loads                                   921
snitch_stores                                  886
fpss_loads                                    1353
fpss_stores                                    338
snitch_avg_load_latency                    10.6743
snitch_occupancy                            0.2059
snitch_fseq_rel_offloads                    0.3496
fseq_yield                                     1.0
fseq_fpu_yield                                 1.0
fpss_section_latency                             0
fpss_avg_fpu_latency                        1.7327
fpss_avg_load_latency                       3.0081
fpss_occupancy                              0.1106
fpss_fpu_occupancy                          0.0498
fpss_fpu_rel_occupancy                      0.4501
cycles                                       27796
total_ipc                                   0.3165
```

Along with minimal performance stats, the execution trace shows each instruction's
dispatch and retirement along the core's cycle timeline: 

```shell
$ grep -B3 -A2 11383 logs/trace_hart_00000000.txt 
11380 M 0x800007a8 flw     ft0, 4008(a5)   #; ft0  <~~ Word[0x00100624]
11381 M 0x800007ac flw     ft1, 4056(a4)   #; ft1  <~~ Word[0x001000dc], (f:lsu) ft0  <-- 2.75
11382 M                                    #; (f:lsu) ft1  <-- 5.5
11383 M 0x800007b0 fadd.s  ft0, ft0, ft1   #; ft0  = 2.75, ft1  = 5.5
11386 M                                    #; (f:fpu) ft0  <-- 8.25
11387 M 0x800007b4 fsw     ft0, -40(a4)    #; 8.25 ~~> Word[0x001000dc]
```

From the decoded trace we can see that in a single element-wise sum iteration we are
spending:

* 2 cycles to load the first operand from L1 (cycles `11380:11381`)
* 2 cycles to load the second operand from L1 (cycles `11381:11382`)
* 3 cycles in total to load all operands (cycles `11380:11382` spent by the *load-store unit*, a.k.a. `lsu`)
* 4 cycles to perform the actual addition (cycles `11383:11386`)
* *some* cycles to store the result back to memory

We can also check that the core is doing what we expect:

```shell
$ grep fadd\.s logs/trace_hart_00000000.txt  | wc -l
338
$ grep fadd\.s logs/trace_hart_00000001.txt  | wc -l
0
```

The core (a.k.a. *hart* in RISC-V jargon) no. 0 did one floating point addition per matrix
element, while all of the other cores did none as they are skipped in the `main` function.
