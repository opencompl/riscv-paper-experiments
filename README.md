# riscv-paper-experiments

## RTL Simulation Quickstart

Let's start by spinning up the `snitch-toolchain` Docker container and mounting
a clone of this repo inside it at `/src`:

```shell
$ git clone https://github.com/opencompl/riscv-paper-experiments.git
$ docker run -ti --volume ${PWD}/riscv-paper-experiments:/src ghcr.io/nazavode/snitch-toolchain bash
```

*Note: `opencompl` members seems not to have enough rights to push packages to the organization's
package registry. The image built from [`snitch/docker/Dockerfile`](snitch/docker/Dockerfile) is currently made available at:
[`ghcr.io/nazavode/snitch-toolchain:latest`](https://github.com/users/nazavode/packages/container/package/snitch-toolchain)*

*Note: if on macOS, be aware that the official Docker app is trash, [OrbStack](https://orbstack.dev/) is highly recommended.*

*Note: if you're running Docker on an architecture other than `x86_64` (e.g.: Apple Silicon),
it's likely that your `docker run` command will complain about the image being `linux/amd64`.
Add the following option to explicitly ask for a specific platform and avoid issues:*

```shell
$docker run --platform linux/amd64 ...
```

To run binaries on Snitch we need to firsly build both the Snitch and MLIR runtimes:

```shell
$ cd /src/snitch/snitch-runtime
$ make install
$ cd /src/snitch/mlir-runtime
$ make install
```

To build a RISC-V executable, start from one of the kernels:

```shell
$ cd /src/kernels/addf/mlir
$ make
$ ls *.x
main_snitch.x
```

The [Makefile](kernels/addf/mlir/Makefile) performs the following steps:

1. `.mlir` -> `.ll.mlir` (a.k.a. MLIR source that uses the LLVM dialect only) via `mlir-opt`
2. `.ll.mlir` -> `.ll` via `mlir-translate`
3. `.ll` -> `.ll12` (a.k.a. LLVM IR backported to a form compatible with LLVM 12) via [`tollvm12.py`](snitch/tollvm12.py)
4. `.ll12` -> `.o` via `clang`
5. `.o` -> `.x` via `lld`, linking both runtimes and using the correct linker script to lay out the ELF file.

Once the ELF executable is ready, we can simulate its execution on a Snitch
cluster via the RTL simulator generated by Verilator:

```shell
$ make run
/opt/snitch-rtl/bin/snitch_cluster.vlt main_snitch.x
Warning: Failed to write binary name to logs/.rtlbinary
Wrote 36 bytes of bootrom to 0x1000
Wrote entry point 0x800006f0 to bootloader slot 0x1020
Wrote 38 bytes of bootdata to 0x1024
[Tracer] Logging Hart          8 to logs/trace_hart_00000008.dasm
[Tracer] Logging Hart          0 to logs/trace_hart_00000000.dasm
[Tracer] Logging Hart          1 to logs/trace_hart_00000001.dasm
[Tracer] Logging Hart          2 to logs/trace_hart_00000002.dasm
[Tracer] Logging Hart          3 to logs/trace_hart_00000003.dasm
[Tracer] Logging Hart          4 to logs/trace_hart_00000004.dasm
[Tracer] Logging Hart          5 to logs/trace_hart_00000005.dasm
[Tracer] Logging Hart          6 to logs/trace_hart_00000006.dasm
[Tracer] Logging Hart          7 to logs/trace_hart_00000007.dasm
$ echo $?
0
```

*Note: while the `main` function is run by all the compute cores in the cluster,
the current startup code **returns the integer return value of the core no. 0 only**,
**return values of cores other than no. 0 are discarded**.*

To disassemble and decode the execution traces:

```shell
$ make traces
$ tail -18 logs/trace_hart_00000000.txt
Performance metrics for section 0 @ (12, 16627):
snitch_loads                                   392
snitch_stores                                  396
fpss_loads                                    1353
fpss_stores                                    338
snitch_avg_load_latency                    11.2653
snitch_occupancy                            0.1071
snitch_fseq_rel_offloads                    0.6335
fseq_yield                                     1.0
fseq_fpu_yield                                 1.0
fpss_section_latency                             0
fpss_avg_fpu_latency                        1.7327
fpss_avg_load_latency                       7.9113
fpss_occupancy                              0.1851
fpss_fpu_occupancy                          0.0833
fpss_fpu_rel_occupancy                      0.4501
cycles                                       16616
total_ipc                                   0.2921
```

Along with minimal performance stats, the execution trace shows each instruction's
dispatch and retirement along the core's cycle timeline: 

```shell
$ grep -B4 -A2 9549 logs/trace_hart_00000000.txt 
9538        M 0x800001e4 flw     ft0, 4088(a4)          #; ft0  <~~ Word[0x800013f4]
9539        M 0x800001e8 flw     ft1, 16(a5)            #; ft1  <~~ Word[0x00100334]
9547        M                                           #; (f:lsu) ft0  <-- 10.25
9548        M                                           #; (f:lsu) ft1  <-- 20.5
9549        M 0x800001ec fadd.s  ft0, ft0, ft1          #; ft0  = 10.25, ft1  = 20.5
9552        M                                           #; (f:fpu) ft0  <-- 30.75
9553        M 0x800001f0 fsw     ft0, 16(a5)            #; 30.75 ~~> Word[0x00100334]
```

From the decoded trace we can see that in a single element-wise sum iteration we are
spending:

* 9 cycles to load the first operand (cycles `9538:9547`)
* 9 cycles to load the second operand (cycles `9539:9548`)
* 10 cycles in total to load all operands (cycles `9538:9548` spent by the *load-store unit*, a.k.a. `lsu`)
* 4 cycles to perform the actual addition (`fpu`)
* *some* cycles to store the result back to memory

We can also check that the core is doing what we expect:

```shell
$ grep fadd\.s logs/trace_hart_00000000.txt  | wc -l
338
$ grep fadd\.s logs/trace_hart_00000001.txt  | wc -l
0
```

The core (a.k.a. *hart* in RISC-V jargon) no. 0 did one floating point addition per matrix
element, while all of the other cores did none as they are skipped in the `main` function.
